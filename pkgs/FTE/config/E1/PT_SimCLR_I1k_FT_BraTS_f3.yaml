# global options
seed: 3
num_workers: 8
batch_size: 128
epochs: 50
prefetch_factor: 1
device: "cuda"
verbose: True
#folds: 1 # Setting folds to a value different from null will do n-fold cross-validation

# dataset options
opmode: "live_cache"
data_format: ".npy"
normalizer: "naive" # naive, 01_clipped_CT, imagenet, windowed
# normalizer_window: [null, null, 0, 1]
shape_desired: [256, 256]
tf_device: "cpu"
no_augs: False # Disable augmentations for debugging purposes?

# model options
model_name: "resnet50"
syncbn: True
model_path: "/Projects/DATL/logs_and_checkpoints/pretraining/sancheck_1k100/encoder_pretrained.tar"

# task (does projection head, decoder, dataset, etc, in hardcoded fashion)
task: "BraTS"

# loss options
optimizer: "AdamW" # Adam, AdamW, LARS (experimental)
lr: 1.0e-3 # learning rate 
weight_decay: 1.0e-5 # (LARS excepts BN and Bias from weight decay)
scheduler: decay
decay_per_epoch: 0.95
scheduling_interval: "epoch" # step or epoch as scheduling steps
w_l: [1, 5, 5, 5] # loss weighting (null <=> [1, ...])
w_m: null       # metrics weighting (null <=> [1, ...])

# logging and checkpointing options
name: "E1/PT_SimCLR_I1k_FT_BraTS_f3" # name of the current run, this is also where things get saved
log_losses: True # log losses