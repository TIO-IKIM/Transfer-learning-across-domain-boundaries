# distributed training
nodes: 1 # 1 for DataParallel
gpus: 1 # DataParallel only wants to work for gpus=1, but captures all available GPUs
nr: 0 # machine nr. in node (0 -- nodes - 1)
dataparallel: True # Use DataParallel instead of DistributedDataParallel
num_workers: 8
prefetch_factor: 2
dataset_dir: "./datasets" # Directory where downloaded demo stuff goes

# train options
seed: 42 # sacred handles automatic seeding when passed in the config
batch_size: 4096
image_size: 256 # SimCLR transforms data to end up with H and W set to image_size
start_epoch: 0
epochs: 100
pretrain: True 

# dataset options
dataset: "Custom_2D"
#dataset_base_dir: "/Projects/Data_Dump/imagenet-1k/ILSVRC/Data/CLS-LOC/train" # Directory where real data lives
dataset_base_dir: "/Projects/Data_Dump/medical_2D/train"
random_subset: 1 # size of random subset, as a fraction of the original size
opmode: "live_cache" #  'disk', 'pre_cache' or 'live_cache'
grayscale: False # Whether or not input data is already single-channel grayscale (like for CTs)
data_format: ".npy"
normalizer: "naive" # naive, 01_clipped_CT, imagenet, windowed
#normalizer_window: [null, null, 0, 1]
shape_desired: [256, 256] # The dataset outputs data of this scale. If None, outputs whatever shape the raw data has
tf_device: "gpu"
loss_device: "gpu"

# model options
resnet: "resnet50"
projection_dim: 256 # "[...] to project the representation to an n-dimensional latent space"

# loss options
optimizer: "LARS" # Adam, AdamW, LARS (experimental)
stepwise_scheduler: "None" # DecayingCosine, CosineAnnealing, Exponential, None
# annealing_time: 0.2 (how long one annealing step takes, as a fraction of epochs)
# decay_per_epoch: 0.5 (by what factor the lr has shrunk after one epoch)
lr: 3.0e-4 # learning rate (The LARS lr is hardcoded to the square root rule from the SimCLR paper. Any adaptive optimizer only uses lr as base lr.)
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10âˆ’6"
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

# logging and checkpointing options
name: "testrange" # Name of the run - Where the pretraining logs and checkpoints live
log_losses: True
low_verbosity: False
enc_save_frequency: 5 # number of epochs between model checkpoints

# reload options
# reload_path: "save" # set to the directory containing `checkpoint_##.tar` 
checkpoint_epoch: 0 # set to checkpoint number
reload: False

# logistic regression options
logistic_batch_size: 256 #Disabled
logistic_epochs: 500 #Disabled